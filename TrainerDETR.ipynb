{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not load the custom kernel for multi-scale deformable attention: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# os.environ['TORCHDYNAMO_VERBOSE'] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights, fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models import MobileNet_V3_Large_Weights, ResNet50_Weights\n",
    "from torchvision.models.detection.rpn import concat_box_prediction_layers\n",
    "from torchvision.models.detection.roi_heads import fastrcnn_loss\n",
    "import transformers\n",
    "from transformers import DetaImageProcessor, DetaForObjectDetection, DetaConfig, DetrImageProcessor, DetrForObjectDetection, DetrConfig, DeformableDetrForObjectDetection, DeformableDetrConfig, DeformableDetrImageProcessor\n",
    "from transformers.models.deta.image_processing_deta import AnnotionFormat\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "from fastai.vision import *\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import RichProgressBar, ModelCheckpoint, EarlyStopping, LearningRateMonitor, RichModelSummary\n",
    "import cv2\n",
    "from pycocotools import coco, cocoeval, _mask\n",
    "from pycocotools import mask as maskUtils\n",
    "from PIL import  Image\n",
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "from logging.config import fileConfig\n",
    "import sys\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "fileConfig(\"logging.ini\")\n",
    "logger = logging.getLogger(\"trainer\")\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = Path(\"/home/vamsik1211/Data/git-repos/ClearquoteProject/exercise-2/dataset/train\")\n",
    "coco_annotations = {}\n",
    "for image_dir in ROOT_DIR.iterdir():\n",
    "    if image_dir.is_dir():\n",
    "        coco_data = coco.COCO(image_dir / \"coco_data.json\")\n",
    "        coco_annotations[image_dir.name] = coco_data\n",
    "\n",
    "# Test Data\n",
    "ROOT_DIR_TEST = Path(\"/home/vamsik1211/Data/git-repos/ClearquoteProject/exercise-2/dataset/test\")\n",
    "coco_annotations_test = {}\n",
    "for image_dir in ROOT_DIR_TEST.iterdir():\n",
    "    if image_dir.is_dir():\n",
    "        coco_data = coco.COCO(image_dir / \"coco_data.json\")\n",
    "        coco_annotations_test[image_dir.name] = coco_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData(Dataset):\n",
    "    def __init__(self, root_dir: Path, coco_annotations: dict[str, coco.COCO], transform=None, device=\"cpu\", processor: DetrImageProcessor = None):\n",
    "        self.root_dir = root_dir\n",
    "        self.coco_annotations = coco_annotations\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "\n",
    "        self.processor = processor\n",
    "\n",
    "    def load_images_paths(self) -> list[Path]:\n",
    "        images = []\n",
    "        for image_dir in self.root_dir.iterdir():\n",
    "            if image_dir.is_dir():\n",
    "                images.extend(list(image_dir.glob(\"**.jpg\")))\n",
    "        return images\n",
    "    \n",
    "    def __len__(self):\n",
    "        # return len(self.images)\n",
    "        total_len = 0\n",
    "        for anns in self.coco_annotations.values():\n",
    "            total_len += len(anns.getImgIds())\n",
    "\n",
    "        return total_len\n",
    "    \n",
    "    def get_idx_coco_section(self, idx: int) -> tuple[str, coco.COCO, int]:\n",
    "        start_len = 0\n",
    "        for folder_name, anns in self.coco_annotations.items():\n",
    "            end_len = start_len + len(anns.getImgIds())\n",
    "            if idx < end_len:\n",
    "                return folder_name, anns, idx - start_len + 1\n",
    "            else:\n",
    "                start_len = end_len\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \n",
    "        pixel_values = [item[0] for item in batch]\n",
    "        # pixel_masks = [item[1] for item in batch]\n",
    "        encoding = self.processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "        labels = [item[1] for item in batch]\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": encoding[\"pixel_values\"],\n",
    "            \"pixel_mask\": encoding[\"pixel_mask\"],\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        \n",
    "            \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # print(\"INDEX\", idx)\n",
    "        folder_name, annotations, image_idx = self.get_idx_coco_section(idx)\n",
    "        image_attributes = annotations.loadImgs([image_idx])[0]\n",
    "        image_path = self.root_dir / folder_name / image_attributes[\"file_name\"]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = np.array(image)\n",
    "\n",
    "        # print(folder_name, image_idx, image_attributes)\n",
    "\n",
    "\n",
    "        # annotations = self.coco_annotations[image_path.parent.name].loadAnns(self.coco_annotations[image_path.parent.name].getAnnIds(image_idx))\n",
    "        annotations = annotations.imgToAnns[image_idx]\n",
    "\n",
    "        # filter annotations to have only LCD\n",
    "        # annotations = [ann for ann in annotations if ann[\"category_id\"] == 1]\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for annotation in annotations:\n",
    "            # bbox = [annotation['bbox'][0], annotation['bbox'][1], annotation['bbox'][0] + annotation['bbox'][2], annotation['bbox'][1] + annotation['bbox'][3]]\n",
    "            bbox = [annotation[\"bbox\"][0], annotation[\"bbox\"][1], annotation[\"bbox\"][2], annotation[\"bbox\"][3]]\n",
    "            boxes.append(bbox)\n",
    "            labels.append(annotation[\"category_id\"])\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed_data = self.transform(image=image, bboxes=boxes, labels=labels)\n",
    "            image = transformed_data[\"image\"]\n",
    "            boxes = transformed_data[\"bboxes\"]\n",
    "            # logger.debug(\"BOXES: %s\", boxes)\n",
    "            # labels = transformed_data[\"labels\"]\n",
    "\n",
    "        # for ann_idx, annotation in enumerate(annotations):\n",
    "        #     annotations[ann_idx][\"bbox\"] = torch.as_tensor(boxes[ann_idx])\n",
    "\n",
    "        annotations = {\n",
    "            \"image_id\": image_idx,\n",
    "            \"annotations\": annotations\n",
    "        }\n",
    "\n",
    "        # image = image/255.\n",
    "        encoding = self.processor(images=image, annotations=annotations, return_tensors=\"pt\")\n",
    "        pixel_value = encoding[\"pixel_values\"].squeeze()\n",
    "        target = encoding[\"labels\"][0]\n",
    "        \n",
    "        return pixel_value, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Resize(720, 1280),\n",
    "    A.ChannelShuffle(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2, ),\n",
    "    A.GaussNoise(p=0.5, var_limit=(10.0, 100.0)),\n",
    "    A.Equalize(p=0.5),\n",
    "    # A.Normalize(),\n",
    "    # ToTensorV2()\n",
    "# ], bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=['LCD', 'M', 'not_touching', 'odometer', 'screen']))\n",
    "], bbox_params=A.BboxParams(format=\"coco\", label_fields=['labels']))\n",
    "\n",
    "transform_test = A.Compose([\n",
    "    A.Resize(720, 1280),\n",
    "    # A.Normalize(),\n",
    "    # ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format=\"coco\", label_fields=['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "def create_timm_backbone(name: str, pretrained: bool = True):\n",
    "    # Load the timm model\n",
    "    timm_model = timm.create_model(name, pretrained=pretrained)\n",
    "\n",
    "    # Remove the classification head to get the backbone\n",
    "    backbone = torch.nn.Sequential(*list(timm_model.children())[:-2])\n",
    "\n",
    "    # Calculate the number of output channels\n",
    "    num_channels = list(backbone.children())[-1].out_channels\n",
    "\n",
    "    return backbone, num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Using device cuda [/tmp/ipykernel_242410/1391204146.py:33]\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCHS = 300\n",
    "MIN_EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "HF_MODEL_NAME = \"facebook/detr-resnet-50\"\n",
    "# HF_MODEL_NAME = \"facebook/detr-resnet-101\"\n",
    "\n",
    "\n",
    "CATEGORIES = coco_annotations_test[list(coco_annotations_test.keys())[0]].cats\n",
    "ID2LABEL = {k: v[\"name\"] for k, v in CATEGORIES.items()}\n",
    "NUM_LABELS = len(CATEGORIES)\n",
    "\n",
    "# LEARNING_RATES = {\n",
    "#     \"backbone\": 1e-4,\n",
    "#     \"others\": 1e-3\n",
    "# }\n",
    "\n",
    "LEARNING_RATES = {\n",
    "    \"backbone\": 1e-4,\n",
    "    \"input_proj\": 1e-4,\n",
    "    \"query_position_embeddings\": 1e-4,\n",
    "    \"encoder\": 1e-3,\n",
    "    \"decoder\": 1e-3,\n",
    "    \"class_labels_classifier\": 1e-3,\n",
    "    \"bbox_predictor\": 1e-3,\n",
    "    \n",
    "}\n",
    "\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device {DEVICE}\")\n",
    "\n",
    "processor = DetrImageProcessor.from_pretrained(HF_MODEL_NAME, device=DEVICE, format=AnnotionFormat.COCO_DETECTION, do_resize=False, do_pad=False, do_rescale=True, revision=\"no_timm\", num_labels=3, ignore_mismatched_sizes=True)\n",
    "\n",
    "odometer_dataset = LoadData(ROOT_DIR, coco_annotations, transform=transform, processor=processor, device=DEVICE)\n",
    "odometer_dataset_test = LoadData(ROOT_DIR_TEST, coco_annotations_test, transform=transform_test, processor=processor, device=DEVICE)\n",
    "\n",
    "odometer_dataloader = DataLoader(odometer_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=odometer_dataset.collate_fn, num_workers=NUM_WORKERS)\n",
    "odometer_dataloader_test = DataLoader(odometer_dataset_test, batch_size=BATCH_SIZE, shuffle=False, collate_fn=odometer_dataset_test.collate_fn, num_workers=NUM_WORKERS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "STEPS_PER_EPOCH = len(odometer_dataset)/BATCH_SIZE\n",
    "if type(STEPS_PER_EPOCH) == float:\n",
    "    STEPS_PER_EPOCH = math.ceil(STEPS_PER_EPOCH)\n",
    "\n",
    "LOG_DIR = \"tb_logs\"\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "CHECKPOINT_FILE_NAME = \"detr_model-{epoch:02d}-{val_loss:.2f}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'LCD', 2: 'odometer'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID2LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 720, 1280])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(odometer_dataset))\n",
    "ret_data = odometer_dataset[78]\n",
    "\n",
    "# Image.fromarray((ret_data[0].permute(1, 2, 0).numpy()*255).astype(np.uint8))\n",
    "ret_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "detr_config = DetrConfig.from_pretrained(HF_MODEL_NAME, num_labels=NUM_LABELS, id2label=ID2LABEL, label2id={v: k for k, v in ID2LABEL.items()}, revision=\"no_timm\", ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detr_model = DetrForObjectDetection.from_pretrained(HF_MODEL_NAME, revision=\"no_timm\", num_labels=2, ignore_mismatched_sizes=True).to(DEVICE)\n",
    "detr_model = DetrForObjectDetection(detr_config).to(DEVICE)\n",
    "\n",
    "for param in detr_model.model.backbone.parameters():\n",
    "    param.requires_grad = True\n",
    "# detr_model = torch.compile(detr_model, backend=\"cudagraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(odometer_dataloader))\n",
    "detr_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = detr_model(batch[\"pixel_values\"].to(DEVICE), batch[\"pixel_mask\"].to(DEVICE), [{k: v.to(DEVICE) for k, v in t.items()} for t in batch[\"labels\"]])\n",
    "\n",
    "# processor.post_process_object_detection(outputs, threshold=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor.post_process_object_detection(outputs, threshold=0.0)\n",
    "# batch[\"pixel_mask\"].min()\n",
    "# outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS, STEP_OUTPUT, TRAIN_DATALOADERS, OptimizerLRScheduler\n",
    "\n",
    "\n",
    "class DetrModelPL(L.LightningModule):\n",
    "    def __init__(self, model: DetrForObjectDetection, \n",
    "                 processor: DetrImageProcessor,\n",
    "                 device: str = \"cpu\",\n",
    "                 learning_rates=LEARNING_RATES,\n",
    "                    weight_decay=WEIGHT_DECAY,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                    max_epochs=MAX_EPOCHS,\n",
    "                    min_epochs=MIN_EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    num_workers=NUM_WORKERS,\n",
    "                    model_name=HF_MODEL_NAME,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        self.save_hyperparameters(\n",
    "            \"learning_rates\",\n",
    "            \"weight_decay\",\n",
    "            \"steps_per_epoch\",\n",
    "            \"max_epochs\",\n",
    "            \"min_epochs\",\n",
    "            \"batch_size\",\n",
    "            \"model_name\"\n",
    "        )\n",
    "        \n",
    "    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "\n",
    "        # optimizer_backbone, optimizer_others = self.optimizers()\n",
    "        (optimizer_model_backbone, optimizer_model_input_projection, \\\n",
    "            optimizer_model_query_position_embeddings, optimizer_model_encoder, \\\n",
    "                optimizer_model_decoder, optimizer_model_class_labels_classifier, \\\n",
    "                    optimizer_model_bbox_predictor) = self.optimizers()\n",
    "        \n",
    "        pixel_values = batch[\"pixel_values\"].to(self.device)\n",
    "        pixel_mask = batch[\"pixel_mask\"].to(self.device)\n",
    "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss_dict = outputs[\"loss_dict\"]\n",
    "        logits = outputs[\"logits\"]\n",
    "        pred_boxes = outputs[\"pred_boxes\"]\n",
    "\n",
    "        if not math.isfinite(loss):\n",
    "            logger.debug(f\"Loss is {loss}, stopping training\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=pixel_values.shape[0])\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(f\"train_{k}\", v.item(), on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=pixel_values.shape[0])\n",
    "\n",
    "        # optimizer_backbone.zero_grad()\n",
    "        # optimizer_others.zero_grad()\n",
    "        optimizer_model_backbone.zero_grad()\n",
    "        optimizer_model_input_projection.zero_grad()\n",
    "        optimizer_model_query_position_embeddings.zero_grad()\n",
    "        optimizer_model_encoder.zero_grad()\n",
    "        optimizer_model_decoder.zero_grad()\n",
    "        optimizer_model_class_labels_classifier.zero_grad()\n",
    "        optimizer_model_bbox_predictor.zero_grad()\n",
    "\n",
    "        self.manual_backward(loss)\n",
    "\n",
    "        # optimizer_backbone.step()\n",
    "        # optimizer_others.step()\n",
    "        optimizer_model_backbone.step()\n",
    "        optimizer_model_input_projection.step()\n",
    "        optimizer_model_query_position_embeddings.step()\n",
    "        optimizer_model_encoder.step()\n",
    "        optimizer_model_decoder.step()\n",
    "        optimizer_model_class_labels_classifier.step()\n",
    "        optimizer_model_bbox_predictor.step()\n",
    "\n",
    "        (lr_scheduler_model_backbone, lr_scheduler_model_input_projection, \\\n",
    "            lr_scheduler_model_query_position_embeddings, lr_scheduler_model_encoder, \\\n",
    "                lr_scheduler_model_decoder, lr_scheduler_model_class_labels_classifier, \\\n",
    "                    lr_scheduler_model_bbox_predictor) = self.lr_schedulers()\n",
    "\n",
    "        # lr_scheduler_backbone.step()\n",
    "        # lr_scheduler_others.step()\n",
    "        \n",
    "        lr_scheduler_model_backbone.step()\n",
    "        lr_scheduler_model_input_projection.step()\n",
    "        lr_scheduler_model_query_position_embeddings.step()\n",
    "        lr_scheduler_model_encoder.step()\n",
    "        lr_scheduler_model_decoder.step()\n",
    "        lr_scheduler_model_class_labels_classifier.step()\n",
    "        lr_scheduler_model_bbox_predictor.step()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        pixel_mask = batch[\"pixel_mask\"]\n",
    "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss_dict = outputs[\"loss_dict\"]\n",
    "        logits = outputs[\"logits\"]\n",
    "        pred_boxes = outputs[\"pred_boxes\"]\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=pixel_values.shape[0])\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(f\"val_{k}\", v.item(), on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=pixel_values.shape[0])\n",
    "\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    # def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        \n",
    "\n",
    "        backbone_params = [p for n, p in self.model.named_parameters() if \"backbone\" in n]\n",
    "        other_params = [p for n, p in self.model.named_parameters() if \"backbone\" not in n]\n",
    "\n",
    "        optimizer_backbone = torch.optim.SGD(backbone_params, lr=LEARNING_RATES[\"backbone\"], momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
    "        optimizer_others = torch.optim.SGD(other_params, lr=LEARNING_RATES[\"others\"], momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer_backbone, max_lr=LEARNING_RATES[\"backbone\"], steps_per_epoch=STEPS_PER_EPOCH, epochs=MAX_EPOCHS)\n",
    "        lr_scheduler_backbone = optim.lr_scheduler.OneCycleLR(optimizer_backbone, max_lr=LEARNING_RATES[\"backbone\"], pct_start=0.15, div_factor=1.5, final_div_factor=100000, base_momentum=0.75, max_momentum=0.95, epochs=MAX_EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)\n",
    "        lr_scheduler_others = optim.lr_scheduler.OneCycleLR(optimizer_others, max_lr=LEARNING_RATES[\"others\"], pct_start=0.15, div_factor=1.5, final_div_factor=100000, base_momentum=0.75, max_momentum=0.95, epochs=MAX_EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)\n",
    "\n",
    "        lr_scheduler_backbone = {\n",
    "            \"scheduler\": lr_scheduler_backbone,\n",
    "            \"interval\": \"step\",\n",
    "            \"name\": \"backbone_lr\",\n",
    "        }\n",
    "        lr_scheduler_others = {\n",
    "            \"scheduler\": lr_scheduler_others,\n",
    "            \"interval\": \"step\",\n",
    "            \"name\": \"others_lr\",\n",
    "        }\n",
    "\n",
    "        return ([optimizer_backbone, optimizer_others], [lr_scheduler_backbone, lr_scheduler_others])\n",
    "    \n",
    "    def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        \n",
    "        optimizer_model_backbone = torch.optim.SGD(self.model.model.backbone.parameters(), lr=LEARNING_RATES[\"backbone\"], weight_decay=WEIGHT_DECAY)\n",
    "        optimizer_model_input_projection = torch.optim.SGD(self.model.model.input_projection.parameters(), lr=LEARNING_RATES[\"input_proj\"], weight_decay=WEIGHT_DECAY)\n",
    "        optimizer_model_query_position_embeddings = torch.optim.SGD(self.model.model.query_position_embeddings.parameters(), lr=LEARNING_RATES[\"query_position_embeddings\"], weight_decay=WEIGHT_DECAY)\n",
    "        optimizer_model_encoder = torch.optim.SGD(self.model.model.encoder.parameters(), lr=LEARNING_RATES[\"encoder\"], weight_decay=WEIGHT_DECAY)\n",
    "        optimizer_model_decoder = torch.optim.SGD(self.model.model.decoder.parameters(), lr=LEARNING_RATES[\"decoder\"], weight_decay=WEIGHT_DECAY)\n",
    "        optimizer_model_class_labels_classifier = torch.optim.SGD(self.model.class_labels_classifier.parameters(), lr=LEARNING_RATES[\"class_labels_classifier\"], weight_decay=WEIGHT_DECAY)\n",
    "        optimizer_model_bbox_predictor = torch.optim.SGD(self.model.bbox_predictor.parameters(), lr=LEARNING_RATES[\"bbox_predictor\"], weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        lr_scheduler_model_backbone = optim.lr_scheduler.OneCycleLR(optimizer_model_backbone, max_lr=LEARNING_RATES[\"backbone\"], pct_start=0.15, div_factor=1.5, final_div_factor=100000, base_momentum=0.75, max_momentum=0.95, epochs=MAX_EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)\n",
    "        lr_scheduler_model_input_projection = optim.lr_scheduler.OneCycleLR(optimizer_model_input_projection, max_lr=LEARNING_RATES[\"input_proj\"], pct_start=0.15, div_factor=1.5, final_div_factor=100000, base_momentum=0.75, max_momentum=0.95, epochs=MAX_EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)\n",
    "        lr_scheduler_model_query_position_embeddings = optim.lr_scheduler.OneCycleLR(optimizer_model_query_position_embeddings, max_lr=LEARNING_RATES[\"query_position_embeddings\"], pct_start=0.15, div_factor=1.5, final_div_factor=100000, base_momentum=0.75, max_momentum=0.95, epochs=MAX_EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)\n",
    "        lr_scheduler_model_encoder = optim.lr_scheduler.OneCycleLR(optimizer_model_encoder, max_lr=LEARNING_RATES[\"encoder\"], pct_start=0.15, div_factor=1.5, final_div_factor=100000, base_momentum=0.75, max_momentum=0.95, epochs=MAX_EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)\n",
    "        lr_scheduler_model_decoder = optim.lr_scheduler.OneCycleLR(optimizer_model_decoder, max_lr=LEARNING_RATES[\"decoder\"], pct_start=0.15, div_factor=1.5, final_div_factor=100000, base_momentum=0.75, max_momentum=0.95, epochs=MAX_EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)\n",
    "        lr_scheduler_model_class_labels_classifier = optim.lr_scheduler.OneCycleLR(optimizer_model_class_labels_classifier, max_lr=LEARNING_RATES[\"class_labels_classifier\"], pct_start=0.15, div_factor=1.5, final_div_factor=100000, base_momentum=0.75, max_momentum=0.95, epochs=MAX_EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)\n",
    "        lr_scheduler_model_bbox_predictor = optim.lr_scheduler.OneCycleLR(optimizer_model_bbox_predictor, max_lr=LEARNING_RATES[\"bbox_predictor\"], pct_start=0.15, div_factor=1.5, final_div_factor=100000, base_momentum=0.75, max_momentum=0.95, epochs=MAX_EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)\n",
    "\n",
    "        lr_scheduler_model_backbone = {\n",
    "            \"scheduler\": lr_scheduler_model_backbone,\n",
    "            \"interval\": \"step\",\n",
    "            \"name\": \"model_backbone_lr\",\n",
    "        }\n",
    "        lr_scheduler_model_input_projection = {\n",
    "            \"scheduler\": lr_scheduler_model_input_projection,\n",
    "            \"interval\": \"step\",\n",
    "            \"name\": \"model_input_projection_lr\",\n",
    "        }\n",
    "        lr_scheduler_model_query_position_embeddings = {\n",
    "            \"scheduler\": lr_scheduler_model_query_position_embeddings,\n",
    "            \"interval\": \"step\",\n",
    "            \"name\": \"model_query_position_embeddings_lr\",\n",
    "        }\n",
    "        lr_scheduler_model_encoder = {\n",
    "            \"scheduler\": lr_scheduler_model_encoder,\n",
    "            \"interval\": \"step\",\n",
    "            \"name\": \"model_encoder_lr\",\n",
    "        }\n",
    "        lr_scheduler_model_decoder = {\n",
    "            \"scheduler\": lr_scheduler_model_decoder,\n",
    "            \"interval\": \"step\",\n",
    "            \"name\": \"model_decoder_lr\",\n",
    "        }\n",
    "        lr_scheduler_model_class_labels_classifier = {\n",
    "            \"scheduler\": lr_scheduler_model_class_labels_classifier,\n",
    "            \"interval\": \"step\",\n",
    "            \"name\": \"model_class_labels_classifier_lr\",\n",
    "        }\n",
    "        lr_scheduler_model_bbox_predictor = {\n",
    "            \"scheduler\": lr_scheduler_model_bbox_predictor,\n",
    "            \"interval\": \"step\",\n",
    "            \"name\": \"model_bbox_predictor_lr\",\n",
    "        }\n",
    "\n",
    "        return ([optimizer_model_backbone, optimizer_model_input_projection, \\\n",
    "                 optimizer_model_query_position_embeddings, optimizer_model_encoder, \\\n",
    "                    optimizer_model_decoder, optimizer_model_class_labels_classifier, \\\n",
    "                        optimizer_model_bbox_predictor], \n",
    "                [lr_scheduler_model_backbone, lr_scheduler_model_input_projection, \\\n",
    "                 lr_scheduler_model_query_position_embeddings, lr_scheduler_model_encoder, \\\n",
    "                    lr_scheduler_model_decoder, lr_scheduler_model_class_labels_classifier, \\\n",
    "                        lr_scheduler_model_bbox_predictor])\n",
    "\n",
    "\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return odometer_dataloader\n",
    "\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return odometer_dataloader_test\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name                                    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0  </span>│ model                                   │ DetrForObjectDetection    │ 41.5 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1  </span>│ model.model                             │ DetrModel                 │ 41.4 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2  </span>│ model.model.backbone                    │ DetrConvModel             │ 23.5 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3  </span>│ model.model.backbone.conv_encoder       │ DetrConvEncoder           │ 23.5 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4  </span>│ model.model.backbone.position_embedding │ DetrSinePositionEmbedding │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5  </span>│ model.model.input_projection            │ Conv2d                    │  524 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6  </span>│ model.model.query_position_embeddings   │ Embedding                 │ 25.6 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7  </span>│ model.model.encoder                     │ DetrEncoder               │  7.9 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8  </span>│ model.model.encoder.layers              │ ModuleList                │  7.9 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9  </span>│ model.model.decoder                     │ DetrDecoder               │  9.5 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 10 </span>│ model.model.decoder.layers              │ ModuleList                │  9.5 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 11 </span>│ model.model.decoder.layernorm           │ LayerNorm                 │    512 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 12 </span>│ model.class_labels_classifier           │ Linear                    │    771 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 13 </span>│ model.bbox_predictor                    │ DetrMLPPredictionHead     │  132 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 14 </span>│ model.bbox_predictor.layers             │ ModuleList                │  132 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 15 </span>│ model.bbox_predictor.layers.0           │ Linear                    │ 65.8 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 16 </span>│ model.bbox_predictor.layers.1           │ Linear                    │ 65.8 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 17 </span>│ model.bbox_predictor.layers.2           │ Linear                    │  1.0 K │\n",
       "└────┴─────────────────────────────────────────┴───────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                                   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0m│ model                                   │ DetrForObjectDetection    │ 41.5 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0m│ model.model                             │ DetrModel                 │ 41.4 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0m│ model.model.backbone                    │ DetrConvModel             │ 23.5 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0m│ model.model.backbone.conv_encoder       │ DetrConvEncoder           │ 23.5 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0m│ model.model.backbone.position_embedding │ DetrSinePositionEmbedding │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0m│ model.model.input_projection            │ Conv2d                    │  524 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0m│ model.model.query_position_embeddings   │ Embedding                 │ 25.6 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0m│ model.model.encoder                     │ DetrEncoder               │  7.9 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0m│ model.model.encoder.layers              │ ModuleList                │  7.9 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0m│ model.model.decoder                     │ DetrDecoder               │  9.5 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0m│ model.model.decoder.layers              │ ModuleList                │  9.5 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0m│ model.model.decoder.layernorm           │ LayerNorm                 │    512 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m12\u001b[0m\u001b[2m \u001b[0m│ model.class_labels_classifier           │ Linear                    │    771 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m13\u001b[0m\u001b[2m \u001b[0m│ model.bbox_predictor                    │ DetrMLPPredictionHead     │  132 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m14\u001b[0m\u001b[2m \u001b[0m│ model.bbox_predictor.layers             │ ModuleList                │  132 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m15\u001b[0m\u001b[2m \u001b[0m│ model.bbox_predictor.layers.0           │ Linear                    │ 65.8 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m16\u001b[0m\u001b[2m \u001b[0m│ model.bbox_predictor.layers.1           │ Linear                    │ 65.8 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m17\u001b[0m\u001b[2m \u001b[0m│ model.bbox_predictor.layers.2           │ Linear                    │  1.0 K │\n",
       "└────┴─────────────────────────────────────────┴───────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 41.5 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 41.5 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 166                                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 41.5 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 41.5 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 166                                                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fb4d25a0fb4b948e68b02f8de2dabe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = DetrModelPL(detr_model, processor)\n",
    "\n",
    "callbacks = [\n",
    "    RichProgressBar(), \n",
    "    LearningRateMonitor(\n",
    "        logging_interval=\"step\",\n",
    "    ),\n",
    "    RichModelSummary(max_depth=4),\n",
    "    ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        filename=CHECKPOINT_FILE_NAME,\n",
    "        save_top_k=3,\n",
    "        # dirpath=\"checkpoints\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "tb_logger = TensorBoardLogger(\n",
    "    save_dir=LOG_DIR,\n",
    "    name=\"detr_model\",\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    min_epochs=MIN_EPOCHS,\n",
    "    accelerator=\"gpu\",\n",
    "    callbacks=callbacks,\n",
    "    logger=tb_logger,\n",
    "    precision=32,\n",
    "    enable_checkpointing=True,\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clearquote",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
